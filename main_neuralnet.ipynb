{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main file, will be copied to neuralnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Epoches: 50 N Batches: 100 Batch size: 500 MOMENTUM? True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a121ed69c042e98a6562ad2a855918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss train: (1.2708495752333355, array([[1.04113045e-03, 7.83713530e-01, 2.37689920e-03, ...,\n",
      "        5.81397393e-03, 9.77143524e-02, 4.16221681e-03],\n",
      "       [5.43782773e-02, 6.12366809e-04, 1.69197016e-03, ...,\n",
      "        1.04701903e-01, 3.75472940e-02, 2.88217619e-01],\n",
      "       [3.86551121e-02, 3.97569050e-02, 3.68312587e-02, ...,\n",
      "        4.46610127e-02, 6.48168031e-01, 7.03986881e-02],\n",
      "       ...,\n",
      "       [1.59643554e-01, 1.19018533e-03, 6.93167731e-03, ...,\n",
      "        4.40717218e-03, 2.92203088e-01, 4.33115673e-01],\n",
      "       [1.16878116e-02, 6.70802961e-02, 2.25320579e-01, ...,\n",
      "        4.72186426e-03, 2.95878586e-01, 3.70878740e-03],\n",
      "       [1.46749844e-01, 1.16460964e-03, 7.51278042e-02, ...,\n",
      "        7.05108899e-04, 1.82037543e-02, 8.90857744e-04]])) loss validate: 1.2604478022301495\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3f2b4e378204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m   \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3f2b4e378204>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, X_train, y_train, X_valid, y_valid, config)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m       \u001b[0;31m# Weight updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3f2b4e378204>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m       \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3f2b4e378204>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, delta)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# This is dE/dw = delta received . dx/dw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# This is dE/db = delta received . 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "config = {}\n",
    "config['layer_specs'] = [784, 50, 10]  # The length of list denotes number of hidden layers; each element denotes number of neurons in that layer; first element is the size of input layer, last element is the size of output layer.\n",
    "config['activation'] = 'sigmoid' # Takes values 'sigmoid', 'tanh' or 'ReLU'; denotes activation function for hidden layers\n",
    "config['batch_size'] = 500  # Number of training samples per batch to be passed to network\n",
    "config['epochs'] = 50  # Number of epochs to train the model\n",
    "config['early_stop'] = True  # Implement early stopping or not\n",
    "config['early_stop_epoch'] = 5  # Number of epochs for which validation loss increases to be counted as overfitting\n",
    "config['L2_penalty'] = 0  # Regularization constant\n",
    "config['momentum'] = True  # Denotes if momentum is to be applied or not\n",
    "config['momentum_gamma'] = 0.9  # Denotes the constant 'gamma' in momentum expression\n",
    "config['learning_rate'] = 0.0001 # Learning rate of gradient descent algorithm\n",
    "\n",
    "def softmax(x):\n",
    "  \"\"\"\n",
    "  Write the code for softmax activation function that takes in a numpy array and returns a numpy array.\n",
    "  \"\"\"\n",
    "  out_exp = np.exp(x)\n",
    "  sum_out_exp = out_exp.sum(axis=1)  \n",
    "  output = out_exp/sum_out_exp[:,None]  \n",
    "  return output\n",
    "\n",
    "def sigmoid(x):\n",
    "  \"\"\"\n",
    "  General Sigmoid function\n",
    "  \"\"\"\n",
    "  return 1./(1. + np.exp(-x))\n",
    "\n",
    "def load_data(fname):\n",
    "  \"\"\"\n",
    "  Write code to read the data and return it as 2 numpy arrays.\n",
    "  Make sure to convert labels to one hot encoded format.\n",
    "  \"\"\"\n",
    "  f = open(fname, 'rb')\n",
    "  data = pickle.load(f)\n",
    "  f.close()\n",
    "  images, labels = data[:, :-1], data[:, -1]\n",
    "\n",
    "  labels = labels.astype(np.int)\n",
    "  onehotlabels = np.zeros((len(labels), labels.max()+1))\n",
    "  onehotlabels[np.arange(len(labels)), labels] = 1\n",
    "  return images, onehotlabels\n",
    "\n",
    "\n",
    "class Activation:\n",
    "  def __init__(self, activation_type = \"sigmoid\"):\n",
    "    self.activation_type = activation_type\n",
    "    self.x = None # Save the input 'x' for sigmoid or tanh or ReLU to this variable since it will be used later for computing gradients.\n",
    "  \n",
    "  def forward_pass(self, a):\n",
    "    if self.activation_type == \"sigmoid\":\n",
    "      return self.sigmoid(a)\n",
    "    \n",
    "    elif self.activation_type == \"tanh\":\n",
    "      return self.tanh(a)\n",
    "    \n",
    "    elif self.activation_type == \"ReLU\":\n",
    "      return self.ReLU(a)\n",
    "  \n",
    "  def backward_pass(self, delta):\n",
    "    if self.activation_type == \"sigmoid\":\n",
    "      grad = self.grad_sigmoid()\n",
    "    \n",
    "    elif self.activation_type == \"tanh\":\n",
    "      grad = self.grad_tanh()\n",
    "    \n",
    "    elif self.activation_type == \"ReLU\":\n",
    "      grad = self.grad_ReLU()\n",
    "    \n",
    "    return grad * delta\n",
    "      \n",
    "  def sigmoid(self, x):\n",
    "    \"\"\"\n",
    "    Write the code for sigmoid activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    output = sigmoid(x)\n",
    "    return output\n",
    "\n",
    "  def tanh(self, x):\n",
    "    \"\"\"\n",
    "    Write the code for tanh activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    output = np.tanh(x)\n",
    "    return output\n",
    "\n",
    "  def ReLU(self, x):\n",
    "    \"\"\"\n",
    "    Write the code for ReLU activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    output = np.maximum(x, 0)\n",
    "    return output\n",
    "\n",
    "  def grad_sigmoid(self):\n",
    "    \"\"\"\n",
    "    Write the code for gradient through sigmoid activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    sigmoid_x = sigmoid(self.x)\n",
    "    grad = sigmoid_x * (1-sigmoid_x)\n",
    "    return grad\n",
    "\n",
    "  def grad_tanh(self):\n",
    "    \"\"\"\n",
    "    Write the code for gradient through tanh activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    tanh_x = np.tanh(self.x)\n",
    "    grad = 1 - (tanh_x * tanh_x)\n",
    "    return grad\n",
    "\n",
    "  def grad_ReLU(self):\n",
    "    \"\"\"\n",
    "    Write the code for gradient through ReLU activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.where(self.x <= 0, 0, 1)\n",
    "    return grad\n",
    "\n",
    "\n",
    "class Layer():\n",
    "  def __init__(self, in_units, out_units):\n",
    "    np.random.seed(42)\n",
    "    self.w = np.random.randn(in_units, out_units)  # Weight matrix\n",
    "    self.b = np.zeros((1, out_units)).astype(np.float32)  # Bias\n",
    "    self.x = None  # Save the input to forward_pass in this\n",
    "    self.a = None  # Save the output of forward pass in this (without activation)\n",
    "    self.d_x = None  # Save the gradient w.r.t x in this (AKA Delta to pass to previous layer = dE/dx)\n",
    "    self.d_w = None  # Save the gradient w.r.t w in this (AKA dE/dw = Delta received . dx/dw)\n",
    "    self.d_b = None  # Save the gradient w.r.t b in this (AKA dE/db = Delta received . 1)\n",
    "\n",
    "  def forward_pass(self, x):\n",
    "    \"\"\"\n",
    "    Write the code for forward pass through a layer. Do not apply activation function here.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    self.a = x @ self.w + self.b\n",
    "    return self.a\n",
    "  \n",
    "  def backward_pass(self, delta):\n",
    "    \"\"\"\n",
    "    Write the code for backward pass. This takes in gradient from its next layer as input,\n",
    "    computes gradient for its weights and the delta to pass to its previous layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # This is dE/dw = delta received . dx/dw\n",
    "    self.d_w = (np.swapaxes(self.x.T[:,:,np.newaxis],1,2) * np.swapaxes(delta[:,:,np.newaxis], 0, 2)).sum(axis=2)\n",
    "\n",
    "    # This is dE/db = delta received . 1\n",
    "    self.d_b = delta.sum(axis=0)\n",
    "                \n",
    "    # This is delta to be passed to previous layer\n",
    "    self.d_x = delta @ self.w.T\n",
    "    return self.d_x\n",
    "\n",
    "      \n",
    "class Neuralnetwork():\n",
    "  def __init__(self, config):\n",
    "    self.layers = []\n",
    "    self.x = None  # Save the input to forward_pass in this\n",
    "    self.y = None  # Save the output vector of model in this\n",
    "    self.targets = None  # Save the targets in forward_pass in this variable\n",
    "    for i in range(len(config['layer_specs']) - 1):\n",
    "      self.layers.append( Layer(config['layer_specs'][i], config['layer_specs'][i+1]) )\n",
    "      \n",
    "      # Unless it's output unit, add Activation layer on top\n",
    "      if i < len(config['layer_specs']) - 2:\n",
    "        self.layers.append(Activation(config['activation']))  \n",
    "    \n",
    "  def forward_pass(self, x, targets=None):\n",
    "    \"\"\"\n",
    "    Write the code for forward pass through all layers of the model and return loss and predictions.\n",
    "    If targets == None, loss should be None. If not, then return the loss computed.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    self.targets = targets\n",
    "    \n",
    "    # Input layer\n",
    "    out = self.layers[0].forward_pass(x)\n",
    "    \n",
    "    # Forward...\n",
    "    for layer in self.layers[1:]:\n",
    "      out = layer.forward_pass(out)\n",
    "      \n",
    "    # Softmax\n",
    "    self.y = softmax(out)\n",
    "        \n",
    "    # Cross-entropy loss\n",
    "    if targets is not None:\n",
    "      return self.loss_func(self.y, targets), self.y\n",
    "    else:    \n",
    "      return None, self.y\n",
    "\n",
    "  def loss_func(self, logits, targets):\n",
    "    '''\n",
    "    find cross entropy loss between logits and targets\n",
    "    '''\n",
    "    output = -(targets * np.log(logits)).sum()/len(targets)\n",
    "    return output\n",
    "    \n",
    "  def backward_pass(self):\n",
    "    '''\n",
    "    implement the backward pass for the whole network. \n",
    "    hint - use previously built functions.\n",
    "    '''\n",
    "    delta = self.targets - self.y\n",
    "    \n",
    "    for layer in reversed(self.layers):\n",
    "      delta = layer.backward_pass(delta)\n",
    "    return delta\n",
    "\n",
    "def trainer(model, X_train, y_train, X_valid, y_valid, config):\n",
    "  \"\"\"\n",
    "  Write the code to train the network. Use values from config to set parameters\n",
    "  such as L2 penalty, number of epochs, momentum, etc.\n",
    "  \"\"\"\n",
    "  \n",
    "  BATCH_SIZE = config['batch_size']\n",
    "  N_EPOCHS = config['epochs']\n",
    "  LEARNING_RATE = config['learning_rate']\n",
    "  \n",
    "  N_BATCHES = len(X_train) // BATCH_SIZE\n",
    "\n",
    "  EPOCHS_THRESHOLD = config['early_stop_epoch']\n",
    "\n",
    "  USE_MOMENTUM = config['momentum']\n",
    "  USE_EARLY_STOP = config['early_stop']\n",
    "  \n",
    "  GAMMA = 0\n",
    "  if USE_MOMENTUM:\n",
    "    GAMMA = config['momentum_gamma']\n",
    "  \n",
    "  print(\"N Epoches:\", N_EPOCHS, \"N Batches:\",N_BATCHES, \"Batch size:\", BATCH_SIZE, \"MOMENTUM?\", USE_MOMENTUM)\n",
    "\n",
    "  best_weight_layers = []\n",
    "  min_loss = float('inf')\n",
    "  prev_loss = float('inf')\n",
    "  consecutive_epochs = 0\n",
    "  \n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "  train_accuracies = []\n",
    "  valid_accuracies = []\n",
    "  \n",
    "  for i_epoch in tqdm(range(N_EPOCHS)):\n",
    "    \n",
    "    def get_shuffle_inds():\n",
    "      shuffled_inds = np.arange(len(X_train))\n",
    "      np.random.shuffle(shuffled_inds)\n",
    "      return shuffled_inds\n",
    "    \n",
    "    shuffled_inds = get_shuffle_inds()\n",
    "        \n",
    "    \n",
    "    velocities_w = {l: np.zeros_like(l.w) for l in model.layers if type(l) is Layer}\n",
    "    velocities_b = {l: np.zeros_like(l.b) for l in model.layers if type(l) is Layer}\n",
    "    \n",
    "    for i_minibatch in range(0, len(X_train), BATCH_SIZE):\n",
    "      X_batch = X_train[shuffled_inds][i_minibatch:i_minibatch + BATCH_SIZE]\n",
    "      y_batch = y_train[shuffled_inds][i_minibatch:i_minibatch + BATCH_SIZE]\n",
    "      \n",
    "      loss, _ = model.forward_pass(X_batch, y_batch)\n",
    "      delta = model.backward_pass()\n",
    "            \n",
    "      # Weight updates\n",
    "      for l in model.layers:\n",
    "        if type(l) is Layer:\n",
    "          \n",
    "          prev_vw = velocities_w[l]\n",
    "          current_vw = GAMMA*prev_vw + LEARNING_RATE * l.d_w\n",
    "          \n",
    "          prev_vb = velocities_b[l]\n",
    "          current_vb = GAMMA*prev_vb + LEARNING_RATE * l.d_b\n",
    "          \n",
    "          velocities_w[l] = current_vw\n",
    "          velocities_b[l] = current_vb\n",
    "          \n",
    "          l.w += current_vw\n",
    "          l.b += current_vb\n",
    "\n",
    "    # RECORD FOR REPORT\n",
    "    loss_train, _ = model.forward_pass(X_train, y_train)\n",
    "    loss_valid, _ = model.forward_pass(X_valid, y_valid)    \n",
    "    print('Epoch:', i_epoch, 'loss train:', loss_train, 'loss validate:', loss_valid)\n",
    "        \n",
    "    train_losses.append(loss_train)\n",
    "    valid_losses.append(loss_valid)\n",
    "    \n",
    "    accuracy_train = test(model, X_train, y_train, config)\n",
    "    accuracy_valid = test(model, X_valid, y_valid, config)\n",
    "    \n",
    "    train_accuracies.append(accuracy_train)\n",
    "    valid_accuracies.append(accuracy_valid)\n",
    "    \n",
    "    if USE_EARLY_STOP:      \n",
    "      if loss_valid < min_loss:\n",
    "        min_loss = loss_valid\n",
    "        best_weight_layers = model.layers\n",
    "      \n",
    "      if loss_valid > prev_loss:\n",
    "        consecutive_epochs += 1\n",
    "        if consecutive_epochs == EPOCHS_THRESHOLD:\n",
    "          model.layers = best_weight_layers\n",
    "          print('Stop training as validation loss increases for {} epochs'.format(EPOCHS_THRESHOLD))\n",
    "          break\n",
    "      else: \n",
    "        consecutive_epochs = 0\n",
    "      \n",
    "      prev_loss = loss_valid\n",
    "  return {\n",
    "    'config': config,\n",
    "    'train_losses': train_losses, \n",
    "    'valid_losses': valid_losses, \n",
    "    'train_accuracies': train_accuracies, \n",
    "    'valid_accuracies': valid_accuracies\n",
    "  }\n",
    "        \n",
    "      \n",
    "  \n",
    "def test(model, X_test, y_test, config):\n",
    "  \"\"\"\n",
    "  Write code to run the model on the data passed as input and return accuracy.\n",
    "  \"\"\"\n",
    "  _, logits = model.forward_pass(X_test)\n",
    "  predictions = np.argmax(logits, axis=1) # inds that we made predictions (0,1,2,3, ...)\n",
    "  \n",
    "  # convert y_test from one-hot to actual target inds\n",
    "  targets = y_test.nonzero()[1]\n",
    "  accuracy = (predictions == targets).sum()/len(targets)\n",
    "  return accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  train_data_fname = 'data/MNIST_train.pkl'\n",
    "  valid_data_fname = 'data/MNIST_valid.pkl'\n",
    "  test_data_fname = 'data/MNIST_test.pkl'\n",
    "  \n",
    "  ### Train the network ###\n",
    "  model = Neuralnetwork(config)\n",
    "  X_train, y_train = load_data(train_data_fname)\n",
    "  X_valid, y_valid = load_data(valid_data_fname)\n",
    "  X_test, y_test = load_data(test_data_fname)\n",
    "  trainer(model, X_train, y_train, X_valid, y_valid, config)\n",
    "  test_acc = test(model, X_test, y_test, config)\n",
    "  print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Epoches: 50 N Batches: 100 Batch size: 500 MOMENTUM? True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51b8a39930d4e7c9ddc0f121e8a96d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss train: 2.7219276019535394 loss validate: 2.7219276019535394\n",
      "Epoch: 1 loss train: 1.0400018927839232 loss validate: 1.0400018927839232\n",
      "Epoch: 2 loss train: 0.655082537216904 loss validate: 0.655082537216904\n",
      "Epoch: 3 loss train: 0.5114151137176339 loss validate: 0.5114151137176339\n",
      "Epoch: 4 loss train: 0.43095124167998894 loss validate: 0.43095124167998894\n",
      "Epoch: 5 loss train: 0.3807140396544462 loss validate: 0.3807140396544462\n",
      "Epoch: 6 loss train: 0.3420400522032177 loss validate: 0.3420400522032177\n",
      "Epoch: 7 loss train: 0.3230462649132012 loss validate: 0.3230462649132012\n",
      "Epoch: 8 loss train: 0.3086830593216563 loss validate: 0.3086830593216563\n",
      "Epoch: 9 loss train: 0.2918876307293926 loss validate: 0.2918876307293926\n",
      "Epoch: 10 loss train: 0.28508823466001043 loss validate: 0.28508823466001043\n",
      "Epoch: 11 loss train: 0.27705885585567175 loss validate: 0.27705885585567175\n",
      "Epoch: 12 loss train: 0.26658389849044956 loss validate: 0.26658389849044956\n",
      "Epoch: 13 loss train: 0.27345934274531447 loss validate: 0.27345934274531447\n",
      "Epoch: 14 loss train: 0.2661209442971475 loss validate: 0.2661209442971475\n",
      "Epoch: 15 loss train: 0.2488698996875451 loss validate: 0.2488698996875451\n",
      "Epoch: 16 loss train: 0.24595443880307102 loss validate: 0.24595443880307102\n",
      "Epoch: 17 loss train: 0.23599291406052056 loss validate: 0.23599291406052056\n",
      "Epoch: 18 loss train: 0.2366379221127855 loss validate: 0.2366379221127855\n",
      "Epoch: 19 loss train: 0.23226237546625084 loss validate: 0.23226237546625084\n",
      "Epoch: 20 loss train: 0.24255113350392918 loss validate: 0.24255113350392918\n",
      "Epoch: 21 loss train: 0.23198061383170843 loss validate: 0.23198061383170843\n",
      "Epoch: 22 loss train: 0.2220511948794545 loss validate: 0.2220511948794545\n",
      "Epoch: 23 loss train: 0.2175172985959519 loss validate: 0.2175172985959519\n",
      "Epoch: 24 loss train: 0.24170942915940194 loss validate: 0.24170942915940194\n",
      "Epoch: 25 loss train: 0.22382535286947974 loss validate: 0.22382535286947974\n",
      "Epoch: 26 loss train: 0.2157822347682737 loss validate: 0.2157822347682737\n",
      "Epoch: 27 loss train: 0.21137266762220466 loss validate: 0.21137266762220466\n",
      "Epoch: 28 loss train: 0.21977567145676752 loss validate: 0.21977567145676752\n",
      "Epoch: 29 loss train: 0.2159489689645651 loss validate: 0.2159489689645651\n",
      "Epoch: 30 loss train: 0.22204388934443695 loss validate: 0.22204388934443695\n",
      "Epoch: 31 loss train: 0.21720052660211464 loss validate: 0.21720052660211464\n",
      "Epoch: 32 loss train: 0.22175566575874442 loss validate: 0.22175566575874442\n",
      "Epoch: 33 loss train: 0.21180524047795257 loss validate: 0.21180524047795257\n",
      "Epoch: 34 loss train: 0.2188580674114264 loss validate: 0.2188580674114264\n",
      "Epoch: 35 loss train: 0.215661719926347 loss validate: 0.215661719926347\n",
      "Epoch: 36 loss train: 0.21244964757541507 loss validate: 0.21244964757541507\n",
      "Epoch: 37 loss train: 0.21221286944728182 loss validate: 0.21221286944728182\n",
      "Epoch: 38 loss train: 0.21557511465114218 loss validate: 0.21557511465114218\n",
      "Epoch: 39 loss train: 0.21794261560829714 loss validate: 0.21794261560829714\n",
      "Epoch: 40 loss train: 0.21462883221149678 loss validate: 0.21462883221149678\n",
      "Epoch: 41 loss train: 0.22196526573361494 loss validate: 0.22196526573361494\n",
      "Epoch: 42 loss train: 0.21852615631919808 loss validate: 0.21852615631919808\n",
      "Epoch: 43 loss train: 0.22378042876691565 loss validate: 0.22378042876691565\n",
      "Epoch: 44 loss train: 0.22750368168455098 loss validate: 0.22750368168455098\n",
      "Epoch: 45 loss train: 0.22803084947220637 loss validate: 0.22803084947220637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-47058613a846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-35f2b5f0d407>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, X_train, y_train, X_valid, y_valid, config)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_minibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m       \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_minibatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_minibatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m       \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_minibatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_minibatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  train_data_fname = 'data/MNIST_train.pkl'\n",
    "  valid_data_fname = 'data/MNIST_valid.pkl'\n",
    "  test_data_fname = 'data/MNIST_test.pkl'\n",
    "  \n",
    "  ### Train the network ###\n",
    "  model = Neuralnetwork(config)\n",
    "  X_train, y_train = load_data(train_data_fname)\n",
    "  X_valid, y_valid = load_data(valid_data_fname)\n",
    "  X_test, y_test = load_data(test_data_fname)\n",
    "  trainer(model, X_train, y_train, X_valid, y_valid, config)\n",
    "  test_acc = test(model, X_test, y_test, config)\n",
    "  print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Check with gradient approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8285714285714286\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_data(test_data_fname)\n",
    "\n",
    "EPSILON = 1e-9\n",
    "\n",
    "def test_approximation_bias(model, layer_ind, unit_ind, epsilon=1e-2):\n",
    "  sample_test = np.random.randint(len(X_test))\n",
    "  X_sample = X_test[0,:].reshape((1,-1))\n",
    "  y_sample = y_test[0,:].reshape((1,-1))\n",
    "\n",
    "  original_w = model.layers[layer_ind].b[0,unit_ind]\n",
    "  \n",
    "  # Approximation\n",
    "  model.layers[layer_ind].b[0,unit_ind] = original_w + epsilon\n",
    "  w_plus_epsilon_loss, _ = model.forward_pass(X_sample, y_sample)\n",
    "  model.layers[layer_ind].b[0,unit_ind] = original_w - epsilon\n",
    "  w_minus_epsilon_loss, _ = model.forward_pass(X_sample, y_sample)\n",
    "  \n",
    "  estimated_dE_dw = (w_plus_epsilon_loss - w_minus_epsilon_loss)/(2*epsilon)\n",
    "  \n",
    "  # Restore\n",
    "  model.layers[layer_ind].b[0,unit_ind] = original_w\n",
    "  \n",
    "  # Actual BPP gradient\n",
    "  model.forward_pass(X_sample, y_sample)\n",
    "  model.backward_pass()\n",
    "  bpp_dE_dw = -model.layers[layer_ind].d_b[unit_ind]/len(X_sample)\n",
    "  \n",
    "  return abs(estimated_dE_dw - bpp_dE_dw), abs(estimated_dE_dw - bpp_dE_dw) <= 9*epsilon**2\n",
    "  \n",
    "# BIAS CHECK\n",
    "checks = []\n",
    "model = Neuralnetwork(config)\n",
    "for layer_i in [0, 2, 4]:\n",
    "  for unit_i in range(model.layers[layer_i].b.shape[1]):\n",
    "    raw_diff, is_close_enough = test_approximation_bias(model, layer_i, unit_i, 1e-4)\n",
    "    checks.append(is_close_enough)\n",
    "    \n",
    "print(sum(checks)/len(checks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4574a4c77d7f4de08c93025d386b7a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4359c6e0a42743539e50a22c32db6a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85620222fbb4712a75e3217a87154f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab44e885b75406a94f99396b6e0e0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3242857142857143\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_data(test_data_fname)\n",
    "\n",
    "def test_approximation_weight(model, layer_ind, unit_in, unit_out, epsilon=1e-2):\n",
    "  sample_test = np.random.randint(len(X_test))\n",
    "  X_sample = X_test[sample_test,:].reshape((1,-1))\n",
    "  y_sample = y_test[sample_test,:].reshape((1,-1))\n",
    "\n",
    "  original_w = model.layers[layer_ind].w[unit_in, unit_out]\n",
    "  \n",
    "  # Approximation\n",
    "  model.layers[layer_ind].w[unit_in, unit_out] = original_w + epsilon\n",
    "  w_plus_epsilon_loss, _ = model.forward_pass(X_sample, y_sample)\n",
    "  model.layers[layer_ind].w[unit_in, unit_out] = original_w - epsilon\n",
    "  w_minus_epsilon_loss, _ = model.forward_pass(X_sample, y_sample)\n",
    "  \n",
    "  estimated_dE_dw = (w_plus_epsilon_loss - w_minus_epsilon_loss)/(2*epsilon)\n",
    "  \n",
    "  # Restore\n",
    "  model.layers[layer_ind].w[unit_in, unit_out] = original_w\n",
    "  \n",
    "  # Actual BPP gradient\n",
    "  model.forward_pass(X_sample, y_sample)\n",
    "  model.backward_pass()\n",
    "  bpp_dE_dw = -model.layers[layer_ind].d_w[unit_in, unit_out]/len(X_sample)\n",
    "  \n",
    "  return abs(estimated_dE_dw - bpp_dE_dw), abs(estimated_dE_dw - bpp_dE_dw) <= 9*epsilon**2\n",
    "  \n",
    "# BIAS CHECK\n",
    "checks = []\n",
    "model = Neuralnetwork(config)\n",
    "for layer_i in tqdm([0, 2, 4]):\n",
    "  for unit_in in tqdm(range(min(model.layers[layer_i].w.shape[0], 100))):\n",
    "    for unit_out in range(min(model.layers[layer_i].w.shape[1], 100)):\n",
    "      raw_diff, is_close_enough = test_approximation_weight(model, layer_i, unit_in, unit_out, 1e-6)\n",
    "      checks.append(is_close_enough)\n",
    "    \n",
    "print(sum(checks)/len(checks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Loss/Accuracy vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(config)\n",
    "  \"\"\"\n",
    "    Train model with given config.\n",
    "    Return train_report that has train/valid losses and accuracies\n",
    "  \"\"\"\n",
    "  train_data_fname = 'data/MNIST_train.pkl'\n",
    "  valid_data_fname = 'data/MNIST_valid.pkl'\n",
    "  test_data_fname = 'data/MNIST_test.pkl'\n",
    "  \n",
    "  ### Train the network ###\n",
    "  model = Neuralnetwork(config)\n",
    "  X_train, y_train = load_data(train_data_fname)\n",
    "  X_valid, y_valid = load_data(valid_data_fname)\n",
    "  X_test, y_test = load_data(test_data_fname)\n",
    "  train_report = trainer(model, X_train, y_train, X_valid, y_valid, config)\n",
    "  test_acc = test(model, X_test, y_test, config)\n",
    "  print(\"Test Accuracy:\", test_acc)\n",
    "  return train_report\n",
    "\n",
    "def plot_from_train_report(train_report):\n",
    "  \"\"\"\n",
    "    Plot two graphs from train_report: 1.train/valid losses vs epoch and 2. train/valid accuracies vs epoch\n",
    "  \"\"\"\n",
    "  \n",
    "  train_losses = train_report['train_losses']\n",
    "  valid_losses = train_report['valid_losses']\n",
    "  x = range(len(train_losses))\n",
    "  \n",
    "  plt.plot(x, train_losses, label='Train losses')\n",
    "  plt.plot(x, valid_losses, label='Validation losses')\n",
    "  plt.set_ylabel('Cross-Entropy Loss')\n",
    "  plt.set_xlabel('Epoch')\n",
    "  plt.legend(loc='best')\n",
    "  plt.title('Train & Validation loss vs. Epoch')\n",
    "  plt.show()\n",
    "  \n",
    "  \n",
    "  train_accs = train_report['train_accuracies']\n",
    "  valid_accs = train_report['valid_accuracies']\n",
    "  x = range(len(train_accs))\n",
    "  \n",
    "  plt.plot(x, train_accs, label='Train accuracies')\n",
    "  plt.plot(x, valid_accs, label='Validation accuracies')\n",
    "  plt.set_ylabel('Accuracies')\n",
    "  plt.set_xlabel('Epoch')\n",
    "  plt.legend(loc='best')\n",
    "  plt.title('Train & Validation accuracy vs. Epoch')\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
