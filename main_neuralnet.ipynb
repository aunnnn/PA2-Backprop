{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Batches:  250 Batch size 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e25483ba3614e6694f24740e2c42c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 0.6553940811135778\n",
      "Epoch: 1 loss: 0.5601473517941171\n",
      "Epoch: 2 loss: 0.4862528791589618\n",
      "Epoch: 3 loss: 0.4452262810945276\n",
      "Epoch: 4 loss: 0.42111847408171\n",
      "Epoch: 5 loss: 0.41542858381066183\n",
      "Epoch: 6 loss: 0.3858189624478935\n",
      "Epoch: 7 loss: 0.34932670438611163\n",
      "Epoch: 8 loss: 0.31427097354997374\n",
      "Epoch: 9 loss: 0.29282778464482057\n",
      "Epoch: 10 loss: 0.2707385761536318\n",
      "Epoch: 11 loss: 0.25947934031121705\n",
      "Epoch: 12 loss: 0.24750761272356314\n",
      "Epoch: 13 loss: 0.23617834719924566\n",
      "Epoch: 14 loss: 0.23574686315535828\n",
      "Epoch: 15 loss: 0.2364457681458223\n",
      "Epoch: 16 loss: 0.2391321238810243\n",
      "Epoch: 17 loss: 0.23064789175290315\n",
      "Epoch: 18 loss: 0.22140325615770487\n",
      "Epoch: 19 loss: 0.21574885869364505\n",
      "Epoch: 20 loss: 0.21202004932558474\n",
      "Epoch: 21 loss: 0.20447383909091033\n",
      "Epoch: 22 loss: 0.19332376835154044\n",
      "Epoch: 23 loss: 0.1856987126102387\n",
      "Epoch: 24 loss: 0.180413119625561\n",
      "Epoch: 25 loss: 0.17239145695746624\n",
      "Epoch: 26 loss: 0.16590773335400438\n",
      "Epoch: 27 loss: 0.15717924787377793\n",
      "Epoch: 28 loss: 0.15006307378125883\n",
      "Epoch: 29 loss: 0.14416867685850016\n",
      "Epoch: 30 loss: 0.13932303565306137\n",
      "Epoch: 31 loss: 0.13505815463007573\n",
      "Epoch: 32 loss: 0.13236796077071983\n",
      "Stop training as validation loss increases for 5 epochs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "config = {}\n",
    "config['layer_specs'] = [784, 100, 100, 10]  # The length of list denotes number of hidden layers; each element denotes number of neurons in that layer; first element is the size of input layer, last element is the size of output layer.\n",
    "config['activation'] = 'sigmoid' # Takes values 'sigmoid', 'tanh' or 'ReLU'; denotes activation function for hidden layers\n",
    "config['batch_size'] = 200  # Number of training samples per batch to be passed to network\n",
    "config['epochs'] = 50  # Number of epochs to train the model\n",
    "config['early_stop'] = True  # Implement early stopping or not\n",
    "config['early_stop_epoch'] = 5  # Number of epochs for which validation loss increases to be counted as overfitting\n",
    "config['L2_penalty'] = 0  # Regularization constant\n",
    "config['momentum'] = True  # Denotes if momentum is to be applied or not\n",
    "config['momentum_gamma'] = 0.9  # Denotes the constant 'gamma' in momentum expression\n",
    "config['learning_rate'] = 0.007 # Learning rate of gradient descent algorithm\n",
    "\n",
    "def softmax(x):\n",
    "  \"\"\"\n",
    "  Write the code for softmax activation function that takes in a numpy array and returns a numpy array.\n",
    "  \"\"\"\n",
    "  out_exp = np.exp(x)\n",
    "  sum_out_exp = np.exp(x).sum(axis=1)  \n",
    "  output = out_exp/sum_out_exp[:,None]  \n",
    "  return output\n",
    "\n",
    "def sigmoid(x):\n",
    "  \"\"\"\n",
    "  General Sigmoid function\n",
    "  \"\"\"\n",
    "  return 1./(1. + np.exp(-x))\n",
    "\n",
    "def load_data(fname):\n",
    "  \"\"\"\n",
    "  Write code to read the data and return it as 2 numpy arrays.\n",
    "  Make sure to convert labels to one hot encoded format.\n",
    "  \"\"\"\n",
    "  f = open(fname, 'rb')\n",
    "  data = pickle.load(f)\n",
    "  f.close()\n",
    "  images, labels = data[:, :-1], data[:, -1]\n",
    "\n",
    "  labels = labels.astype(np.int)\n",
    "  onehotlabels = np.zeros((len(labels), labels.max()+1))\n",
    "  onehotlabels[np.arange(len(labels)), labels] = 1\n",
    "  return images, onehotlabels\n",
    "\n",
    "\n",
    "class Activation:\n",
    "  def __init__(self, activation_type = \"sigmoid\"):\n",
    "    self.activation_type = activation_type\n",
    "    self.x = None # Save the input 'x' for sigmoid or tanh or ReLU to this variable since it will be used later for computing gradients.\n",
    "  \n",
    "  def forward_pass(self, a):\n",
    "    if self.activation_type == \"sigmoid\":\n",
    "      return self.sigmoid(a)\n",
    "    \n",
    "    elif self.activation_type == \"tanh\":\n",
    "      return self.tanh(a)\n",
    "    \n",
    "    elif self.activation_type == \"ReLU\":\n",
    "      return self.ReLU(a)\n",
    "  \n",
    "  def backward_pass(self, delta):\n",
    "    if self.activation_type == \"sigmoid\":\n",
    "      grad = self.grad_sigmoid()\n",
    "    \n",
    "    elif self.activation_type == \"tanh\":\n",
    "      grad = self.grad_tanh()\n",
    "    \n",
    "    elif self.activation_type == \"ReLU\":\n",
    "      grad = self.grad_ReLU()\n",
    "    \n",
    "    return grad * delta\n",
    "      \n",
    "  def sigmoid(self, x):\n",
    "    \"\"\"\n",
    "    Write the code for sigmoid activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    output = sigmoid(x)\n",
    "    return output\n",
    "\n",
    "  def tanh(self, x):\n",
    "    \"\"\"\n",
    "    Write the code for tanh activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    output = np.tanh(x)\n",
    "    return output\n",
    "\n",
    "  def ReLU(self, x):\n",
    "    \"\"\"\n",
    "    Write the code for ReLU activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    output = np.maximum(x, 0)\n",
    "    return output\n",
    "\n",
    "  def grad_sigmoid(self):\n",
    "    \"\"\"\n",
    "    Write the code for gradient through sigmoid activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    sigmoid_x = sigmoid(self.x)\n",
    "    grad = sigmoid_x * (1-sigmoid_x)\n",
    "    return grad\n",
    "\n",
    "  def grad_tanh(self):\n",
    "    \"\"\"\n",
    "    Write the code for gradient through tanh activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    tanh_x = np.tanh(self.x)\n",
    "    grad = 1 - (tanh_x * tanh_x)\n",
    "    return grad\n",
    "\n",
    "  def grad_ReLU(self):\n",
    "    \"\"\"\n",
    "    Write the code for gradient through ReLU activation function that takes in a numpy array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.where(self.x <= 0, 0, 1)\n",
    "    return grad\n",
    "\n",
    "\n",
    "class Layer():\n",
    "  def __init__(self, in_units, out_units):\n",
    "    np.random.seed(42)\n",
    "    self.w = np.random.randn(in_units, out_units)  # Weight matrix\n",
    "    self.b = np.zeros((1, out_units)).astype(np.float32)  # Bias\n",
    "    self.x = None  # Save the input to forward_pass in this\n",
    "    self.a = None  # Save the output of forward pass in this (without activation)\n",
    "    self.d_x = None  # Save the gradient w.r.t x in this (AKA Delta to pass to previous layer = dE/dx)\n",
    "    self.d_w = None  # Save the gradient w.r.t w in this (AKA dE/dw = Delta received . dx/dw)\n",
    "    self.d_b = None  # Save the gradient w.r.t b in this (AKA dE/db = Delta received . 1)\n",
    "\n",
    "  def forward_pass(self, x):\n",
    "    \"\"\"\n",
    "    Write the code for forward pass through a layer. Do not apply activation function here.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    self.a = x @ self.w + self.b\n",
    "    return self.a\n",
    "  \n",
    "  def backward_pass(self, delta):\n",
    "    \"\"\"\n",
    "    Write the code for backward pass. This takes in gradient from its next layer as input,\n",
    "    computes gradient for its weights and the delta to pass to its previous layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # This is dE/dw = delta received . dx/dw\n",
    "    self.d_w = (np.swapaxes(self.x.T[:,:,np.newaxis],1,2) * np.swapaxes(delta[:,:,np.newaxis], 0, 2)).sum(axis=2)\n",
    "\n",
    "    # This is dE/db = delta received . 1\n",
    "    self.d_b = delta.sum(axis=0)\n",
    "                \n",
    "    # This is delta to be passed to previous layer\n",
    "    self.d_x = delta @ self.w.T\n",
    "    return self.d_x\n",
    "\n",
    "      \n",
    "class Neuralnetwork():\n",
    "  def __init__(self, config):\n",
    "    self.layers = []\n",
    "    self.x = None  # Save the input to forward_pass in this\n",
    "    self.y = None  # Save the output vector of model in this\n",
    "    self.targets = None  # Save the targets in forward_pass in this variable\n",
    "    for i in range(len(config['layer_specs']) - 1):\n",
    "      self.layers.append( Layer(config['layer_specs'][i], config['layer_specs'][i+1]) )\n",
    "      \n",
    "      # Unless it's output unit, add Activation layer on top\n",
    "      if i < len(config['layer_specs']) - 2:\n",
    "        self.layers.append(Activation(config['activation']))  \n",
    "    \n",
    "  def forward_pass(self, x, targets=None):\n",
    "    \"\"\"\n",
    "    Write the code for forward pass through all layers of the model and return loss and predictions.\n",
    "    If targets == None, loss should be None. If not, then return the loss computed.\n",
    "    \"\"\"\n",
    "    self.x = x\n",
    "    self.targets = targets\n",
    "    \n",
    "    # Input layer\n",
    "    out = self.layers[0].forward_pass(x)\n",
    "    \n",
    "    # Forward...\n",
    "    for layer in self.layers[1:]:\n",
    "      out = layer.forward_pass(out)\n",
    "      \n",
    "    # Softmax\n",
    "    self.y = softmax(out)\n",
    "        \n",
    "    # Cross-entropy loss\n",
    "    if targets is not None:\n",
    "      return self.loss_func(self.y, targets), self.y\n",
    "    else:    \n",
    "      return None, self.y\n",
    "\n",
    "  def loss_func(self, logits, targets):\n",
    "    '''\n",
    "    find cross entropy loss between logits and targets\n",
    "    '''\n",
    "    output = -(targets * np.log(logits)).sum()/len(targets)\n",
    "    return output\n",
    "    \n",
    "  def backward_pass(self):\n",
    "    '''\n",
    "    implement the backward pass for the whole network. \n",
    "    hint - use previously built functions.\n",
    "    '''\n",
    "    delta = self.targets - self.y\n",
    "    \n",
    "    for layer in reversed(self.layers):\n",
    "      delta = layer.backward_pass(delta)\n",
    "    return delta\n",
    "\n",
    "def trainer(model, X_train, y_train, X_valid, y_valid, config):\n",
    "  \"\"\"\n",
    "  Write the code to train the network. Use values from config to set parameters\n",
    "  such as L2 penalty, number of epochs, momentum, etc.\n",
    "  \"\"\"\n",
    "  \n",
    "  BATCH_SIZE = config['batch_size']\n",
    "  N_EPOCHS = config['epochs']\n",
    "  LEARNING_RATE = config['learning_rate']\n",
    "  \n",
    "  N_BATCHES = len(X_train) // BATCH_SIZE\n",
    "\n",
    "  EPOCHS_THRESHOLD = config['early_stop_epoch']\n",
    "\n",
    "  GAMMA = 1\n",
    "  if config['momentum']:\n",
    "    GAMMA = config['momentum_gamma']\n",
    "  \n",
    "  print(\"N Batches: \",N_BATCHES, \"Batch size\", BATCH_SIZE)\n",
    "\n",
    "  best_weight_layers = []\n",
    "  min_loss = float('inf')\n",
    "  prev_loss = float('inf')\n",
    "  consecutive_epochs = 0\n",
    "  \n",
    "  for i_epoch in tqdm(range(N_EPOCHS)):\n",
    "    for i_minibatch in range(0, len(X_train), BATCH_SIZE):\n",
    "      X_batch = X_train[i_minibatch:i_minibatch + BATCH_SIZE]\n",
    "      y_batch = y_train[i_minibatch:i_minibatch + BATCH_SIZE]\n",
    "      \n",
    "      loss, y = model.forward_pass(X_batch, y_batch)\n",
    "      delta = model.backward_pass()\n",
    "            \n",
    "      # Weight updates\n",
    "      for l in model.layers:\n",
    "        if type(l) is Layer:\n",
    "          l.w += GAMMA*LEARNING_RATE * l.d_w\n",
    "          l.b += GAMMA*LEARNING_RATE * l.d_b\n",
    "\n",
    "    print('Epoch:', i_epoch, 'loss:', loss)\n",
    "    \n",
    "    if config['early_stop']:\n",
    "      loss_valid, _ = model.forward_pass(X_valid, y_valid)\n",
    "      if loss_valid < min_loss:\n",
    "        min_loss = loss_valid\n",
    "        best_weight_layers = model.layers\n",
    "      \n",
    "      if loss_valid > prev_loss:\n",
    "        consecutive_epochs += 1\n",
    "        if consecutive_epochs == EPOCHS_THRESHOLD:\n",
    "          model.layers = best_weight_layers\n",
    "          print('Stop training as validation loss increases for {} epochs'.format(EPOCHS_THRESHOLD))\n",
    "          return\n",
    "      else: \n",
    "        consecutive_epochs = 0\n",
    "      \n",
    "      prev_loss = loss_valid\n",
    "        \n",
    "      \n",
    "  \n",
    "def test(model, X_test, y_test, config):\n",
    "  \"\"\"\n",
    "  Write code to run the model on the data passed as input and return accuracy.\n",
    "  \"\"\"\n",
    "  _, logits = model.forward_pass(X_test)\n",
    "  predictions = np.argmax(logits, axis=1) # inds that we made predictions (0,1,2,3, ...)\n",
    "  \n",
    "  # convert y_test from one-hot to actual target inds\n",
    "  targets = y_test.nonzero()[1]\n",
    "  accuracy = (predictions == targets).sum()/len(targets)\n",
    "  return accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  train_data_fname = 'data/MNIST_train.pkl'\n",
    "  valid_data_fname = 'data/MNIST_valid.pkl'\n",
    "  test_data_fname = 'data/MNIST_test.pkl'\n",
    "  \n",
    "  ### Train the network ###\n",
    "  model = Neuralnetwork(config)\n",
    "  X_train, y_train = load_data(train_data_fname)\n",
    "  X_valid, y_valid = load_data(valid_data_fname)\n",
    "  X_test, y_test = load_data(test_data_fname)\n",
    "  trainer(model, X_train, y_train, X_valid, y_valid, config)\n",
    "  test_acc = test(model, X_test, y_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9126"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Check with gradient approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8285714285714286\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_data(test_data_fname)\n",
    "\n",
    "EPSILON = 1e-9\n",
    "\n",
    "def test_approximation_bias(model, layer_ind, unit_ind, epsilon=1e-2):\n",
    "  sample_test = np.random.randint(len(X_test))\n",
    "  X_sample = X_test[0,:].reshape((1,-1))\n",
    "  y_sample = y_test[0,:].reshape((1,-1))\n",
    "\n",
    "  original_w = model.layers[layer_ind].b[0,unit_ind]\n",
    "  \n",
    "  # Approximation\n",
    "  model.layers[layer_ind].b[0,unit_ind] = original_w + epsilon\n",
    "  w_plus_epsilon_loss, _ = model.forward_pass(X_sample, y_sample)\n",
    "  model.layers[layer_ind].b[0,unit_ind] = original_w - epsilon\n",
    "  w_minus_epsilon_loss, _ = model.forward_pass(X_sample, y_sample)\n",
    "  \n",
    "  estimated_dE_dw = (w_plus_epsilon_loss - w_minus_epsilon_loss)/(2*epsilon)\n",
    "  \n",
    "  # Restore\n",
    "  model.layers[layer_ind].b[0,unit_ind] = original_w\n",
    "  \n",
    "  # Actual BPP gradient\n",
    "  model.forward_pass(X_sample, y_sample)\n",
    "  model.backward_pass()\n",
    "  bpp_dE_dw = -model.layers[layer_ind].d_b[unit_ind]/len(X_sample)\n",
    "  \n",
    "  return abs(estimated_dE_dw - bpp_dE_dw), abs(estimated_dE_dw - bpp_dE_dw) <= 9*epsilon**2\n",
    "  \n",
    "# BIAS CHECK\n",
    "checks = []\n",
    "model = Neuralnetwork(config)\n",
    "for layer_i in [0, 2, 4]:\n",
    "  for unit_i in range(model.layers[layer_i].b.shape[1]):\n",
    "    raw_diff, is_close_enough = test_approximation_bias(model, layer_i, unit_i, 1e-4)\n",
    "    checks.append(is_close_enough)\n",
    "    \n",
    "print(sum(checks)/len(checks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4574a4c77d7f4de08c93025d386b7a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4359c6e0a42743539e50a22c32db6a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85620222fbb4712a75e3217a87154f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab44e885b75406a94f99396b6e0e0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3242857142857143\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_data(test_data_fname)\n",
    "\n",
    "def test_approximation_weight(model, layer_ind, unit_in, unit_out, epsilon=1e-2):\n",
    "  sample_test = np.random.randint(len(X_test))\n",
    "  X_sample = X_test[sample_test,:].reshape((1,-1))\n",
    "  y_sample = y_test[sample_test,:].reshape((1,-1))\n",
    "\n",
    "  original_w = model.layers[layer_ind].w[unit_in, unit_out]\n",
    "  \n",
    "  # Approximation\n",
    "  model.layers[layer_ind].w[unit_in, unit_out] = original_w + epsilon\n",
    "  w_plus_epsilon_loss, _ = model.forward_pass(X_sample, y_sample)\n",
    "  model.layers[layer_ind].w[unit_in, unit_out] = original_w - epsilon\n",
    "  w_minus_epsilon_loss, _ = model.forward_pass(X_sample, y_sample)\n",
    "  \n",
    "  estimated_dE_dw = (w_plus_epsilon_loss - w_minus_epsilon_loss)/(2*epsilon)\n",
    "  \n",
    "  # Restore\n",
    "  model.layers[layer_ind].w[unit_in, unit_out] = original_w\n",
    "  \n",
    "  # Actual BPP gradient\n",
    "  model.forward_pass(X_sample, y_sample)\n",
    "  model.backward_pass()\n",
    "  bpp_dE_dw = -model.layers[layer_ind].d_w[unit_in, unit_out]/len(X_sample)\n",
    "  \n",
    "  return abs(estimated_dE_dw - bpp_dE_dw), abs(estimated_dE_dw - bpp_dE_dw) <= 9*epsilon**2\n",
    "  \n",
    "# BIAS CHECK\n",
    "checks = []\n",
    "model = Neuralnetwork(config)\n",
    "for layer_i in tqdm([0, 2, 4]):\n",
    "  for unit_in in tqdm(range(min(model.layers[layer_i].w.shape[0], 100))):\n",
    "    for unit_out in range(min(model.layers[layer_i].w.shape[1], 100)):\n",
    "      raw_diff, is_close_enough = test_approximation_weight(model, layer_i, unit_in, unit_out, 1e-6)\n",
    "      checks.append(is_close_enough)\n",
    "    \n",
    "print(sum(checks)/len(checks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
